<!DOCTYPE html>
<!-- 
  Generated by Config-Driven Academic Website Template
  Author: Sixun Dong (ironieser)
  Repository: https://github.com/Ironieser/ironieser.github.io
  License: MIT
-->
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Publications - Sixun Dong</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
</head>
<body>
    <!-- Navigation -->
    <header class="header">
        <nav class="nav">
            <div class="nav-container">
                <a href="index.html" class="nav-link " >Bio</a>
                <a href="publications.html" class="nav-link active" >Publications</a>
                <a href="blog.html" class="nav-link " >Blog</a>
                <a href="files/CV_Dong.pdf" class="nav-link " target="_blank">CV(PDF)</a>
            </div>
        </nav>
    </header>

    <!-- Main Content -->
    <main class="main">
        <!-- Page Header -->
        <section class="page-header">
            <div class="container">
                <div class="page-header-content">
                    <h1 class="page-title-left">Publications</h1>
                    <div class="research-intro">
                        <p>I develop <strong>multimodal perception and generation systems</strong> that integrate vision, motion, audio, 3D, and language. My research explores weakly supervised video understanding, efficient vision-language models, and agent-centric multimodal generation for embodied AI systems.</p>
                    </div>
                    
                    <!-- Summary Stats Bar -->
                    <div class="publication-stats-bar">
                        <span class="stat-item">10+ publications</span> <span class="stat-divider">‚Ä¢</span> <span class="stat-item">3 top-tier venues (CVPR, WACV, IJCAI)</span> <span class="stat-divider">‚Ä¢</span> <span class="stat-item">1 oral presentation</span>
                    </div>
                </div>
            </div>
        </section>

        <!-- Publications -->
        <section class="section">
            <div class="container">
                
            <div class="year-group">
                <h3 class="year-title">2025</h3>
                <div class="publications-list">
                    
                <div class="publication-item">
                    <img src="teaser/underreview.jpg" alt="Teaching Time Series to See and Speak: Forecasting with Aligned Visual and Textual Perspectives" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                    <div class="publication-content">
                        <p class="publication-title"><span class="publication-venue-under-review">Under Review</span> Teaching Time Series to See and Speak: Forecasting with Aligned Visual and Textual Perspectives</p>
                        <p class="publication-authors"><span class="author-highlight">Sixun Dong</span>, Wei Fan, Teresa Wu, Yanjie Fu</p>
                        <p class="publication-links"><i class="ai ai-arxiv"></i> <span class="coming-soon">Paper (Coming Soon)</span></p>
                    </div>
                </div>
                <div class="publication-item">
                    <img src="teaser/underreview.jpg" alt="TimesFrame: Multi-Variable Time Series is a Video of Numerical Data" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                    <div class="publication-content">
                        <p class="publication-title"><span class="publication-venue-under-review">Under Review</span> TimesFrame: Multi-Variable Time Series is a Video of Numerical Data</p>
                        <p class="publication-authors"><span class="author-highlight">Sixun Dong</span>, Nanxu Gong, Haoyue Bai, Xinyuan Wang, Wangyang Ying, Wei Fan, Yanjie Fu</p>
                        <p class="publication-links"><i class="ai ai-arxiv"></i> <span class="coming-soon">Paper (Coming Soon)</span></p>
                    </div>
                </div>
                <div class="publication-item">
                    <img src="teaser/agentic.jpg" alt="Agentic Feature Augmentation: Unifying Selection and Generation with Teaming, Planning, and Memories" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                    <div class="publication-content">
                        <p class="publication-title"><span class="publication-venue-under-review">Arxiv'2505</span> Agentic Feature Augmentation: Unifying Selection and Generation with Teaming, Planning, and Memories</p>
                        <p class="publication-authors">Nanxu Gong*, <span class="author-highlight">Sixun Dong*</span>, Haoyue Bai, Xinyuan Wang, Wangyang Ying, Yanjie Fu</p>
                        <p class="publication-links"><i class="ai ai-arxiv"></i> <a href="https://arxiv.org/abs/2505.15076" target="_blank">Paper</a></p>
                    </div>
                </div>
                <div class="publication-item">
                    <img src="teaser/sculp.jpg" alt="Sculpting Features from Noise: Reward-Guided Hierarchical Diffusion for Task-Optimal Feature Transformation" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                    <div class="publication-content">
                        <p class="publication-title"><span class="publication-venue-under-review">Arxiv'2505</span> Sculpting Features from Noise: Reward-Guided Hierarchical Diffusion for Task-Optimal Feature Transformation</p>
                        <p class="publication-authors">Nanxu Gong, Zijun Li, <span class="author-highlight">Sixun Dong</span>, Haoyue Bai, Wangyang Ying, Xinyuan Wang, Yanjie Fu</p>
                        <p class="publication-links"><i class="ai ai-arxiv"></i> <a href="https://arxiv.org/abs/2505.15152" target="_blank">Paper</a></p>
                    </div>
                </div>
                <div class="publication-item">
                    <img src="teaser/underreview.jpg" alt="MECT: From Multimodal Knowledge Acquisition To Contrastive Embedding Construction For Generative Feature Transformation" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                    <div class="publication-content">
                        <p class="publication-title"><span class="publication-venue-under-review">Under Review</span> MECT: From Multimodal Knowledge Acquisition To Contrastive Embedding Construction For Generative Feature Transformation</p>
                        <p class="publication-authors">Nanxu Gong, <span class="author-highlight">Sixun Dong</span>, Haoyue Bai, Wangyang Ying, Yanjie Fu</p>
                        <p class="publication-links"><i class="ai ai-arxiv"></i> <span class="coming-soon">Paper (Coming Soon)</span></p>
                    </div>
                </div>
                <div class="publication-item">
                    <img src="teaser/agentFt.jpg" alt="Unsupervised feature transformation via in-context generation, generator-critic llm agents, and duet-play teaming" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                    <div class="publication-content">
                        <p class="publication-title"><span class="publication-venue">IJCAI 2025</span> Unsupervised feature transformation via in-context generation, generator-critic llm agents, and duet-play teaming</p>
                        <p class="publication-authors">Nanxu Gong, Xinyuan Wang, Wangyang Ying, Haoyue Bai, <span class="author-highlight">Sixun Dong</span>, Haifeng Chen, Yanjie Fu</p>
                        <p class="publication-links"><i class="ai ai-arxiv"></i> <a href="https://arxiv.org/abs/2504.21304" target="_blank">Paper</a> / <i class="fab fa-github"></i> <a href="https://github.com/NanxuGong/LPFG" target="_blank">Code</a></p>
                    </div>
                </div>
                </div>
            </div>
            <div class="year-group">
                <h3 class="year-title">2024</h3>
                <div class="publications-list">
                    
                <div class="publication-item">
                    <img src="teaser/mmtool.jpg" alt="MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                    <div class="publication-content">
                        <p class="publication-title"><span class="publication-venue">WACV 2024</span> MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning</p>
                        <p class="publication-authors">Chenyu Wang, Weixin Luo, <span class="author-highlight">Sixun Dong</span>, Xiaohua Xuan, Zhengxin Li, Lin Ma, Shenghua Gao</p>
                        <p class="publication-links"><i class="ai ai-arxiv"></i> <a href="https://arxiv.org/abs/2401.10727" target="_blank">Paper</a> / <i class="fab fa-github"></i> <a href="https://github.com/MLLM-Tool/MLLM-Tool" target="_blank">Code</a></p>
                    </div>
                </div>
                <div class="publication-item">
                    <img src="teaser/room.jpg" alt="RoomDesigner: Encoding Anchor-latents for Style-consistent and Shape-compatible Indoor Scene Generation" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                    <div class="publication-content">
                        <p class="publication-title"><span class="publication-venue">3DV 2024</span> RoomDesigner: Encoding Anchor-latents for Style-consistent and Shape-compatible Indoor Scene Generation</p>
                        <p class="publication-authors">Yiqun Zhao, Zibo Zhao, Jing Li, <span class="author-highlight">Sixun Dong</span>, Shenghua Gao</p>
                        <p class="publication-links"><i class="ai ai-arxiv"></i> <a href="https://arxiv.org/abs/2310.10027" target="_blank">Paper</a> / <i class="fab fa-github"></i> <a href="https://github.com/zhao-yiqun/RoomDesigner" target="_blank">Code</a></p>
                    </div>
                </div>
                </div>
            </div>
            <div class="year-group">
                <h3 class="year-title">2023</h3>
                <div class="publications-list">
                    
                <div class="publication-item">
                    <img src="teaser/weaksvr.jpg" alt="Weakly Supervised Video Representation Learning with Unaligned Text for Sequential Videos" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                    <div class="publication-content">
                        <p class="publication-title"><span class="publication-venue">CVPR 2023</span> Weakly Supervised Video Representation Learning with Unaligned Text for Sequential Videos</p>
                        <p class="publication-authors"><span class="author-highlight">Sixun Dong*</span>, Huazhang Hu*, Dongze Lian, Weixin Luo, Yicheng Qian, Shenghua Gao</p>
                        <p class="publication-links"><i class="ai ai-arxiv"></i> <a href="https://arxiv.org/abs/2303.12370" target="_blank">Paper</a> / <i class="fab fa-github"></i> <a href="https://github.com/svip-lab/WeakSVR" target="_blank">Code</a> / <i class="fab fa-youtube"></i> <a href="https://www.youtube.com/watch?v=AqozSRYP7Pc" target="_blank">YouTube</a> / <i class="fas fa-tv"></i> <a href="https://www.bilibili.com/video/BV1AW4y1R7um" target="_blank">Bilibili</a> / <i class="fas fa-book-open"></i> <a href="https://zhuanlan.zhihu.com/p/617926257" target="_blank">Áü•‰πé</a></p>
                    </div>
                </div>
                </div>
            </div>
            <div class="year-group">
                <h3 class="year-title">2022</h3>
                <div class="publications-list">
                    
                <div class="publication-item">
                    <img src="teaser/transrac.jpg" alt="TransRAC: Encoding Multi-scale Temporal Correlation with Transformers for Repetitive Action Counting" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                    <div class="publication-content">
                        <p class="publication-title"><span class="publication-venue">CVPR 2022</span><span class="publication-venue-oral">üèÜ Oral</span> TransRAC: Encoding Multi-scale Temporal Correlation with Transformers for Repetitive Action Counting</p>
                        <p class="publication-authors">Huazhang Hu*, <span class="author-highlight">Sixun Dong*</span>, Yiqun Zhao, Dongze Lian, Zhengxin Li, Shenghua Gao</p>
                        <p class="publication-links"><i class="ai ai-arxiv"></i> <a href="https://arxiv.org/abs/2204.01018" target="_blank">Paper</a> / <i class="fab fa-github"></i> <a href="https://github.com/SvipRepetitionCounting/TransRAC" target="_blank">Code</a> / <i class="fas fa-database"></i> <a href="https://svip-lab.github.io/dataset/RepCount_dataset.html" target="_blank">Dataset</a> / <i class="fab fa-youtube"></i> <a href="https://youtu.be/SFpUS9mHHpk" target="_blank">YouTube</a> / <i class="fas fa-tv"></i> <a href="https://www.bilibili.com/video/BV1B94y1S7oP" target="_blank">Bilibili</a> / <i class="fas fa-book-open"></i> <a href="https://zhuanlan.zhihu.com/p/543376943" target="_blank">Áü•‰πé</a></p>
                    </div>
                </div>
                </div>
            </div>
            <div class="year-group">
                <h3 class="year-title">Survey Papers</h3>
                <div class="publications-list">
                    
                <div class="publication-item">
                    <img src="teaser/underreview.jpg" alt="Towards Data-Centric AI: A Comprehensive Survey of Traditional, Reinforcement, and Generative Approaches for Tabular Data Transformation" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                    <div class="publication-content">
                        <p class="publication-title"><span class="publication-venue-under-review">Arxiv'2501</span> Towards Data-Centric AI: A Comprehensive Survey of Traditional, Reinforcement, and Generative Approaches for Tabular Data Transformation</p>
                        <p class="publication-authors">Dongjie Wang, Yanyong Huang, Wangyang Ying, Haoyue Bai, Nanxu Gong, Xinyuan Wang, <span class="author-highlight">Sixun Dong</span>, Tao Zhe, Kunpeng Liu, Meng Xiao, et al.</p>
                        <p class="publication-links"><i class="ai ai-arxiv"></i> <a href="https://arxiv.org/abs/2501.10555" target="_blank">Paper</a></p>
                    </div>
                </div>
                <div class="publication-item">
                    <img src="teaser/underreview.jpg" alt="A Survey on Data-Centric AI: Tabular Learning from Reinforcement Learning and Generative AI Perspective" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                    <div class="publication-content">
                        <p class="publication-title"><span class="publication-venue-under-review">Arxiv'2502</span> A Survey on Data-Centric AI: Tabular Learning from Reinforcement Learning and Generative AI Perspective</p>
                        <p class="publication-authors">Wangyang Ying, Cong Wei, Nanxu Gong, Xinyuan Wang, Haoyue Bai, Arun Vignesh Malarkkan, <span class="author-highlight">Sixun Dong</span>, Dongjie Wang, Denghui Zhang, Yanjie Fu</p>
                        <p class="publication-links"><i class="ai ai-arxiv"></i> <a href="https://arxiv.org/abs/2502.08828" target="_blank">Paper</a></p>
                    </div>
                </div>
                </div>
            </div>
            </div>
        </section>
    </main>

    
    <footer class="footer">
        <div class="container">
            <!-- Visitor Map Section -->
            <div class="visitor-map-section">
                <div class="visitor-map-container">
                    <!-- Visitor Map Widget -->
                    <div class="visitor-map">
                        <!-- ClustrMaps Widget -->
                        <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=r_cMMykDPAdqK2GTahWbR__mtnzcj9svUgejZ86OXnU&cl=ffffff&w=a"></script>
                    </div>
                </div>
            </div>

            <div class="footer-stats">
                <div class="stats-item">
                    <i class="fas fa-map-marker-alt"></i>
                    <span id="visitor-location">Loading...</span>
                </div>
                <div class="stats-item">
                    <i class="fas fa-clock"></i>
                    Last updated: <span id="last-updated"></span>
                </div>
            </div>
            <p>&copy; 2025 Sixun Dong. All rights reserved.</p>
        </div>
    </footer>
    
    
    <script>
        // Get visitor location using IP API
        async function getVisitorLocation() {
            try {
                const response = await fetch('https://ipapi.co/json/');
                const data = await response.json();
                const location = `${data.city}, ${data.country_name}`;
                document.getElementById('visitor-location').textContent = location;
            } catch (error) {
                document.getElementById('visitor-location').textContent = 'Unknown';
            }
        }
        
        // Set last updated time
        function setLastUpdated() {
            const now = new Date();
            const formatted = now.toLocaleDateString('en-US', { 
                year: 'numeric', 
                month: 'short', 
                day: 'numeric' 
            });
            document.getElementById('last-updated').textContent = formatted;
        }
        
        // Initialize on page load
        document.addEventListener('DOMContentLoaded', function() {
            getVisitorLocation();
            setLastUpdated();
        });
    </script>
</body>
</html>
