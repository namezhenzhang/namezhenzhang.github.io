#!/usr/bin/env python3
"""
Google Scholar Publications Sync Script
æœ¬åœ°æµ‹è¯•ç‰ˆæœ¬ - ç”¨äºéªŒè¯ScholaråŒæ­¥åŠŸèƒ½
"""

import requests
import json
import re
from bs4 import BeautifulSoup
import time

# ä½ çš„Google Scholarç”¨æˆ·ID
SCHOLAR_USER_ID = "j71Y2-4AAAAJ"

def fetch_scholar_publications():
    """ä»Google Scholarè·å–å‡ºç‰ˆç‰©åˆ—è¡¨"""
    print("ğŸ” Starting Google Scholar sync...")
    
    url = f"https://scholar.google.com/citations?user={SCHOLAR_USER_ID}&hl=en&sortby=pubdate"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        print(f"ğŸ“– Fetching from: {url}")
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # æŸ¥æ‰¾å‡ºç‰ˆç‰©è¡¨æ ¼
        pub_table = soup.find('table', {'id': 'gsc_a_t'})
        if not pub_table:
            print("âŒ Could not find publications table")
            return []
        
        publications = []
        rows = pub_table.find_all('tr', class_='gsc_a_tr')
        
        for row in rows:
            try:
                # æå–æ ‡é¢˜
                title_elem = row.find('a', class_='gsc_a_at')
                if not title_elem:
                    continue
                title = title_elem.text.strip()
                
                # æå–ä½œè€…
                author_elem = row.find('div', class_='gs_gray')
                authors = author_elem.text.strip() if author_elem else ""
                
                # æå–ä¼šè®®/æœŸåˆŠ
                venue_elems = row.find_all('div', class_='gs_gray')
                venue = venue_elems[1].text.strip() if len(venue_elems) > 1 else ""
                
                # æå–å¹´ä»½
                year_elem = row.find('td', class_='gsc_a_y')
                if year_elem:
                    year_span = year_elem.find('span')
                    if year_span and year_span.text.strip().isdigit():
                        year = int(year_span.text.strip())
                    else:
                        year = 2024
                else:
                    year = 2024
                
                # æå–å¼•ç”¨æ•°
                cite_elem = row.find('a', class_='gsc_a_c')
                citations = int(cite_elem.text.strip()) if cite_elem and cite_elem.text.strip().isdigit() else 0
                
                publications.append({
                    'title': title,
                    'authors': [a.strip() for a in authors.split(',')],
                    'venue': venue,
                    'year': year,
                    'citations': citations
                })
                
            except Exception as e:
                print(f"âš ï¸  Error parsing row: {e}")
                continue
        
        print(f"ğŸ“š Found {len(publications)} publications from Scholar")
        return publications
        
    except Exception as e:
        print(f"âŒ Error fetching from Scholar: {e}")
        return []

def parse_venue_info(venue):
    """è§£æä¼šè®®/æœŸåˆŠä¿¡æ¯"""
    venue_map = {
        'CVPR': {'type': 'conference', 'fullName': 'CVPR'},
        'ICCV': {'type': 'conference', 'fullName': 'ICCV'},
        'ECCV': {'type': 'conference', 'fullName': 'ECCV'},
        'NeurIPS': {'type': 'conference', 'fullName': 'NeurIPS'},
        'ICML': {'type': 'conference', 'fullName': 'ICML'},
        'ICLR': {'type': 'conference', 'fullName': 'ICLR'},
        'AAAI': {'type': 'conference', 'fullName': 'AAAI'},
        'IJCAI': {'type': 'conference', 'fullName': 'IJCAI'},
        'WACV': {'type': 'conference', 'fullName': 'WACV'},
        '3DV': {'type': 'conference', 'fullName': '3DV'},
    }
    
    # æ£€æŸ¥å·²çŸ¥ä¼šè®®
    for key, value in venue_map.items():
        if key.lower() in venue.lower():
            return value
    
    # å¤„ç†arXiv
    if 'arxiv' in venue.lower():
        return {'type': 'preprint', 'fullName': 'arXiv'}
    
    # é»˜è®¤ä¸ºä¼šè®®
    return {'type': 'conference', 'fullName': venue}

def check_duplicate_across_all_years(new_pub, existing_config):
    """è·¨å¹´ä»½æ™ºèƒ½é‡å¤æ£€æµ‹"""
    import re
    
    # æ£€æŸ¥æ‰€æœ‰å¹´ä»½çš„æ‰€æœ‰è®ºæ–‡
    all_existing_pubs = []
    for year, pubs in existing_config.get('publications', {}).items():
        if isinstance(pubs, list):
            all_existing_pubs.extend(pubs)
    
    for existing in all_existing_pubs:
        # 1. å®Œå…¨æ ‡é¢˜åŒ¹é…
        if existing['title'].lower() == new_pub['title'].lower():
            print(f"ğŸ” Found exact title match: \"{new_pub['title']}\"")
            return True
        
        # 2. æ ‡é¢˜ç›¸ä¼¼åº¦æ£€æµ‹ï¼ˆå»é™¤æ ‡ç‚¹ç¬¦å·åæ¯”è¾ƒï¼‰
        clean_new = re.sub(r'[^a-z0-9\s]', '', new_pub['title'].lower()).strip()
        clean_existing = re.sub(r'[^a-z0-9\s]', '', existing['title'].lower()).strip()
        if clean_new == clean_existing:
            print(f"ğŸ” Found similar title: \"{new_pub['title']}\" vs \"{existing['title']}\"")
            return True
        
        # 3. æ ¸å¿ƒè¯åŒ¹é…ï¼ˆæ ‡é¢˜é•¿åº¦>20å­—ç¬¦æ—¶ï¼‰
        if len(new_pub['title']) > 20 and len(existing['title']) > 20:
            new_words = [w for w in clean_new.split() if len(w) > 3]
            existing_words = [w for w in clean_existing.split() if len(w) > 3]
            common_words = [w for w in new_words if w in existing_words]
            
            # å¦‚æœ85%ä»¥ä¸Šçš„é‡è¦è¯æ±‡ç›¸åŒï¼Œè®¤ä¸ºæ˜¯é‡å¤
            if len(common_words) / max(len(new_words), len(existing_words)) > 0.85:
                print(f"ğŸ” Found word overlap match: \"{new_pub['title']}\" vs \"{existing['title']}\"")
                return True
        
        # 4. ä½œè€…+å…³é”®è¯åŒ¹é…ï¼ˆä¸ä¾èµ–å¹´ä»½ï¼Œä½†è¦æ›´ä¸¥æ ¼é¿å…è¯¯åˆ¤ï¼‰
        # æ£€æŸ¥å…±åŒä½œè€…
        new_authors = [a.lower().strip() for a in new_pub['authors']]
        existing_authors = [a.lower().strip() for a in existing.get('authors', [])]
        
        common_authors = []
        for na in new_authors:
            for ea in existing_authors:
                if len(na) > 2 and len(ea) > 2 and (na in ea or ea in na):
                    common_authors.append(na)
                    break
        
        # å¦‚æœæœ‰å…±åŒä½œè€…ä¸”æ ‡é¢˜æœ‰é‡å è¯æ±‡ï¼Œå¯èƒ½æ˜¯é‡å¤
        # ä½†è¦æ›´ä¸¥æ ¼ï¼šéœ€è¦æ›´å¤šçš„æ ‡é¢˜é‡å å’Œæ›´é«˜çš„ä½œè€…é‡å æ¯”ä¾‹
        if len(common_authors) >= 3:  # è‡³å°‘3ä¸ªå…±åŒä½œè€…
            title_overlap = len([w for w in clean_new.split() 
                               if len(w) > 4 and w in clean_existing])
            
            # å¯¹äºsurveyè®ºæ–‡ï¼Œéœ€è¦æ›´ä¸¥æ ¼çš„åŒ¹é…æ¡ä»¶
            is_survey_new = 'survey' in clean_new.lower()
            is_survey_existing = 'survey' in clean_existing.lower()
            
            if is_survey_new and is_survey_existing:
                # Surveyè®ºæ–‡éœ€è¦æ›´é«˜çš„ç›¸ä¼¼åº¦é˜ˆå€¼
                # éœ€è¦è‡³å°‘70%çš„æ ‡é¢˜è¯æ±‡é‡å æ‰è®¤ä¸ºæ˜¯é‡å¤
                new_words = clean_new.split()
                existing_words = clean_existing.split()
                overlap_ratio = title_overlap / max(len(new_words), len(existing_words))
                
                if overlap_ratio >= 0.7:
                    print(f"ğŸ” Found survey paper match: \"{new_pub['title']}\" vs \"{existing['title']}\" (common authors: {len(common_authors)}, title overlap: {title_overlap}, ratio: {overlap_ratio:.2f})")
                    return True
                else:
                    print(f"ğŸ“ Survey papers with similar authors but different topics: \"{new_pub['title'][:50]}...\" vs \"{existing['title'][:50]}...\" (overlap ratio: {overlap_ratio:.2f})")
            else:
                # ésurveyè®ºæ–‡ä½¿ç”¨åŸæ¥çš„æ¡ä»¶
                if title_overlap >= 5:
                    print(f"ğŸ” Found author+title match: \"{new_pub['title']}\" vs \"{existing['title']}\" (common authors: {len(common_authors)}, title overlap: {title_overlap})")
                    return True
    
    return False

def update_existing_publication(existing, scholar_data):
    """åªæ›´æ–°éæ‰‹åŠ¨ç¼–è¾‘çš„åŸºç¡€ä¿¡æ¯ï¼Œä¿æŠ¤ç”¨æˆ·çš„æ‰‹åŠ¨ä¿®æ”¹"""
    updated = False
    
    # 1. æ›´æ–°å¼•ç”¨æ•°ç›¸å…³çš„featuredçŠ¶æ€ï¼ˆå¦‚æœç”¨æˆ·æ²¡æœ‰æ‰‹åŠ¨è®¾ç½®ï¼‰
    should_be_featured = scholar_data['citations'] > 10
    if should_be_featured and not existing.get('featured', False):
        existing['featured'] = True
        updated = True
    
    # 2. åªåœ¨æ˜æ˜¾æ˜¯é»˜è®¤å€¼æ—¶æ›´æ–°venueä¿¡æ¯
    if existing.get('venue') in ['Conference', 'Journal', 'arXiv'] or \
       existing.get('venue_type') == 'conference':
        venue_info = parse_venue_info(scholar_data['venue'])
        if existing.get('venue') != venue_info['fullName']:
            existing['venue'] = venue_info['fullName']
            existing['venue_type'] = venue_info['type']
            updated = True
    
    # 3. åªåœ¨é“¾æ¥æ˜¯é»˜è®¤å ä½ç¬¦æ—¶æ›´æ–°
    links = existing.get('links', [])
    has_default_link = any(
        link.get('url') in ['#', ''] or link.get('name') == 'Paper (Coming Soon)'
        for link in links
    )
    
    if has_default_link:
        for link in links:
            if link.get('url') in ['#', '']:
                link['url'] = '#'  # Scholaré“¾æ¥é€šå¸¸éœ€è¦æ‰‹åŠ¨è·å–
                link['name'] = 'Paper'
                updated = True
                break
    
    # 4. ä¿æŠ¤æ‰‹åŠ¨è®¾ç½®çš„teaserå›¾ç‰‡ï¼Œä¸è¦†ç›–
    # æ‰‹åŠ¨ç»´æŠ¤çš„è®ºæ–‡å›¾ç‰‡è·¯å¾„é€šå¸¸ä¸æ˜¯ teaser/preprint.jpg
    # åªæœ‰è‡ªåŠ¨åŒæ­¥çš„è®ºæ–‡æ‰ä½¿ç”¨é»˜è®¤çš„ preprint.jpg
    
    # 5. æ·»åŠ å¼•ç”¨æ•°ä¿¡æ¯ï¼ˆä½œä¸ºå…ƒæ•°æ®ï¼‰
    if scholar_data['citations'] > 0:
        existing['_scholar_citations'] = scholar_data['citations']
        existing['_scholar_last_updated'] = time.strftime('%Y-%m-%d')
    
    return updated

def extract_year_from_arxiv_url(url):
    """ä»arXiv URLä¸­æå–å¹´ä»½"""
    import re
    if not url or 'arxiv' not in url.lower():
        return None
    
    # arXiv URLæ ¼å¼: https://arxiv.org/abs/YYMM.NNNNN æˆ– https://arxiv.org/abs/math-ph/YYMMnnn
    # æ–°æ ¼å¼ (2007å¹´4æœˆå): YYMM.NNNNN
    new_format = re.search(r'arxiv\.org/abs/(\d{4})\.', url)
    if new_format:
        yymm = new_format.group(1)
        year = int(yymm[:2])
        # 2007å¹´4æœˆåçš„æ ¼å¼ï¼ŒYYæ˜¯å¹´ä»½çš„åä¸¤ä½
        if year >= 7:  # 07-99 è¡¨ç¤º 2007-2099
            return 2000 + year
        else:  # 00-06 è¡¨ç¤º 2100-2106 (æœªæ¥)
            return 2100 + year
    
    # æ—§æ ¼å¼ (2007å¹´4æœˆå‰): subject-class/YYMMnnn
    old_format = re.search(r'arxiv\.org/abs/[a-z-]+/(\d{4})', url)
    if old_format:
        yymm = old_format.group(1)
        year = int(yymm[:2])
        # æ—§æ ¼å¼ï¼Œ91-06è¡¨ç¤º1991-2006ï¼Œ07-99è¡¨ç¤º2007-2099
        if year >= 91:
            return 1900 + year
        else:
            return 2000 + year
    
    return None

def smart_year_detection(pub, venue_info):
    """æ™ºèƒ½å¹´ä»½æ£€æµ‹ï¼Œå¤šæºå¹´ä»½ä¿¡æ¯"""
    import re
    current_year = 2025
    
    # 1. å°è¯•ä»arXivé“¾æ¥æå–å¹´ä»½ï¼ˆæœ€å‡†ç¡®ï¼‰
    arxiv_year = None
    if 'venue' in pub and pub['venue']:
        # æ£€æŸ¥venueå­—æ®µä¸­æ˜¯å¦åŒ…å«arXiv IDä¿¡æ¯
        # åŒ¹é…æ ¼å¼ï¼šarXiv:YYMM.NNNNN, arxiv.org/abs/YYMM.NNNNN, arXiv YYMM.NNNNN
        arxiv_match = re.search(r'arxiv[:\s/]*(?:abs/)?(\d{4})\.', pub['venue'].lower())
        if arxiv_match:
            yymm = arxiv_match.group(1)
            year = int(yymm[:2])
            # 2007å¹´4æœˆåçš„æ ¼å¼ï¼ŒYYæ˜¯å¹´ä»½çš„åä¸¤ä½
            if year >= 7:  # 07-99 è¡¨ç¤º 2007-2099
                arxiv_year = 2000 + year
            else:  # 00-06 è¡¨ç¤º 2100-2106 (æœªæ¥)
                arxiv_year = 2100 + year
    
    # 2. å°è¯•ä»venueä¿¡æ¯ä¸­æå–å¹´ä»½
    venue_year = None
    if venue_info['fullName']:
        year_match = re.search(r'20\d{2}', venue_info['fullName'])
        if year_match:
            venue_year = int(year_match.group())
    
    # 3. Scholaræä¾›çš„å¹´ä»½
    scholar_year = pub['year']
    
    # 4. æ™ºèƒ½é€‰æ‹©ä¼˜å…ˆçº§ï¼šarXiv > venue > Scholar
    if arxiv_year and 2000 <= arxiv_year <= current_year:
        print(f"ğŸ“… Using arXiv year {arxiv_year} for '{pub['title'][:50]}...'")
        return arxiv_year
    elif venue_year and 2020 <= venue_year <= current_year:
        print(f"ğŸ“… Using venue year {venue_year} for '{pub['title'][:50]}...'")
        return venue_year
    elif 2020 <= scholar_year <= current_year:
        print(f"ğŸ“… Using Scholar year {scholar_year} for '{pub['title'][:50]}...'")
        return scholar_year
    else:
        # éƒ½ä¸åˆç†ï¼Œä½¿ç”¨å½“å‰å¹´ä»½
        print(f"âš ï¸  No reliable year found for '{pub['title'][:50]}...', using current year {current_year}")
        return current_year

def normalize_author_names(authors, existing_config):
    """æ ‡å‡†åŒ–ä½œè€…å§“åï¼Œå°†ç¼©å†™è½¬æ¢ä¸ºå…¨å"""
    author_mapping = {}
    
    # ä»ä¸ªäººä¿¡æ¯ä¸­è·å–ä¸»è¦ä½œè€…çš„å…¨å
    if existing_config.get('personal') and existing_config['personal'].get('name'):
        full_name = existing_config['personal']['name']
        name_parts = full_name.split()
        if len(name_parts) >= 2:
            first_name = name_parts[0]
            last_name = name_parts[-1]
            
            # åˆ›å»ºå¯èƒ½çš„ç¼©å†™å½¢å¼
            abbreviations = [
                f"{first_name[0]} {last_name}",  # S Dong
                f"{first_name[0]}. {last_name}",  # S. Dong
                f"{first_name[0]}{last_name}",   # SDong
                first_name,                      # Sixun
                last_name                        # Dong
            ]
            
            for abbrev in abbreviations:
                author_mapping[abbrev.lower()] = full_name
    
    # ä»ç°æœ‰è®ºæ–‡ä¸­å­¦ä¹ å…¶ä»–ä½œè€…çš„å…¨åæ˜ å°„
    for year, pubs in existing_config.get('publications', {}).items():
        if isinstance(pubs, list):
            for pub in pubs:
                if pub.get('authors'):
                    for author in pub['authors']:
                        if len(author) > 3:  # è®¤ä¸ºæ˜¯å…¨å
                            name_parts = author.split()
                            if len(name_parts) >= 2:
                                first_name = name_parts[0]
                                last_name = name_parts[-1]
                                
                                # æ·»åŠ ç¼©å†™æ˜ å°„
                                abbreviations = [
                                    f"{first_name[0]} {last_name}",
                                    f"{first_name[0]}. {last_name}",
                                    f"{first_name[0]}{last_name}"
                                ]
                                
                                for abbrev in abbreviations:
                                    author_mapping[abbrev.lower()] = author
    
    # åº”ç”¨å§“åæ ‡å‡†åŒ–
    normalized_authors = []
    for author in authors:
        clean_author = author.strip()
        lower_author = clean_author.lower()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜ å°„çš„å…¨å
        if lower_author in author_mapping:
            normalized_name = author_mapping[lower_author]
            print(f"ğŸ“ Normalized author: \"{clean_author}\" â†’ \"{normalized_name}\"")
            normalized_authors.append(normalized_name)
        else:
            normalized_authors.append(clean_author)
    
    return normalized_authors

def convert_to_config_format(scholar_pubs, existing_config):
    """è½¬æ¢ä¸ºconfig.jsonæ ¼å¼"""
    print("ğŸ”„ Converting Scholar data to config format...")
    
    publications_by_year = {}
    
    for pub in scholar_pubs:
        venue_info = parse_venue_info(pub['venue'])
        
        # æ™ºèƒ½å¹´ä»½æ£€æµ‹
        smart_year = smart_year_detection(pub, venue_info)
        year = str(smart_year)
        
        if year not in publications_by_year:
            publications_by_year[year] = []
        
        # æ ‡å‡†åŒ–ä½œè€…å§“å
        normalized_authors = normalize_author_names(pub['authors'], existing_config)
        
        # è·¨å¹´ä»½æ™ºèƒ½é‡å¤æ£€æµ‹
        pub_with_normalized_authors = pub.copy()
        pub_with_normalized_authors['authors'] = normalized_authors
        exists = check_duplicate_across_all_years(pub_with_normalized_authors, existing_config)
        
        if not exists:
            # æ–°è®ºæ–‡ï¼šæ·»åŠ åŸºç¡€ä¿¡æ¯
            config_pub = {
                'title': pub['title'],
                'authors': normalized_authors,
                'venue': venue_info['fullName'],
                'venue_type': venue_info['type'],
                'image': "teaser/preprint.jpg",  # è‡ªåŠ¨åŒæ­¥çš„è®ºæ–‡ç»Ÿä¸€ä½¿ç”¨preprintå›¾ç‰‡
                'auto_sync': True,  # æ ‡è®°ä¸ºè‡ªåŠ¨åŒæ­¥ï¼Œåˆ é™¤æ­¤æ ‡è®°åä¸å†è‡ªåŠ¨æ›´æ–°
                'links': [
                    {
                        'name': 'Paper',
                        'url': '#',  # éœ€è¦æ‰‹åŠ¨æ·»åŠ é“¾æ¥
                        'icon': 'ai ai-arxiv'
                    }
                ]
            }
            
            # é«˜å¼•ç”¨è®ºæ–‡æ ‡è®°ä¸ºfeatured
            if pub['citations'] > 10:
                config_pub['featured'] = True
            
            publications_by_year[year].append(config_pub)
            print(f"âœ… Added new: {pub['title']} ({year}) [Scholar year: {pub['year']}]")
        else:
            # å·²å­˜åœ¨çš„è®ºæ–‡ï¼šè·³è¿‡ï¼Œä¸æ·»åŠ é‡å¤
            print(f"ğŸ”„ Skipped duplicate: {pub['title']} (already exists)")
    
    return publications_by_year

def update_config():
    """æ›´æ–°config.jsonæ–‡ä»¶"""
    try:
        print("ğŸ“– Reading current config.json...")
        with open('config.json', 'r', encoding='utf-8') as f:
            config_data = json.load(f)
        
        print("ğŸ” Fetching publications from Google Scholar...")
        scholar_pubs = fetch_scholar_publications()
        
        if not scholar_pubs:
            print("âš ï¸  No publications found, skipping update")
            return
        
        print("ğŸ”„ Merging with existing publications...")
        new_pubs = convert_to_config_format(scholar_pubs, config_data)
        
        # æ›´æ–°ç°æœ‰è®ºæ–‡çš„ä½œè€…å§“åï¼ˆå¦‚æœå…è®¸è‡ªåŠ¨åŒæ­¥ï¼‰
        updated_count = 0
        for year, pubs in config_data.get('publications', {}).items():
            if isinstance(pubs, list):
                for pub in pubs:
                    if pub.get('auto_sync') != False and pub.get('authors'):
                        original_authors = pub['authors'][:]
                        normalized_authors = normalize_author_names(pub['authors'], config_data)
                        if original_authors != normalized_authors:
                            pub['authors'] = normalized_authors
                            print(f"ğŸ“ Updated authors for: {pub['title']}")
                            updated_count += 1
        
        # åˆå¹¶æ–°å‡ºç‰ˆç‰©
        added_count = 0
        for year, pubs in new_pubs.items():
            if year not in config_data['publications']:
                config_data['publications'][year] = []
            
            for new_pub in pubs:
                # ä½¿ç”¨è·¨å¹´ä»½é‡å¤æ£€æµ‹
                exists = check_duplicate_across_all_years(new_pub, config_data)
                
                if not exists:
                    config_data['publications'][year].append(new_pub)
                    print(f"âœ… Added: {new_pub['title']} ({year})")
                    added_count += 1
                else:
                    print(f"ğŸ”„ Skipped duplicate: {new_pub['title']} ({year})")
        
        if added_count > 0 or updated_count > 0:
            print("ğŸ’¾ Saving updated config.json...")
            with open('config.json', 'w', encoding='utf-8') as f:
                json.dump(config_data, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ‰ Scholar sync completed! Added {added_count} new publications, updated {updated_count} existing publications.")
        else:
            print("â„¹ï¸  No changes to make.")
        
    except Exception as e:
        print(f"âŒ Error updating config: {e}")

if __name__ == "__main__":
    update_config() 