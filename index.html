<!DOCTYPE html>
<!-- 
  Generated by Config-Driven Academic Website Template
  Author: Sixun Dong (ironieser)
  Repository: https://github.com/Ironieser/ironieser.github.io
  License: MIT
-->
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sixun Dong (Ironieser) - Academic Homepage</title>
    
    <!-- SEO Meta Tags -->
    <meta name="description" content="PhD Student in Computer Science at Arizona State University, focusing on multimodal AI systems, computer vision, and machine learning">
    <meta name="keywords" content="computer vision, multimodal AI, machine learning, deep learning, artificial intelligence, academic research, Sixun Dong, Ironieser">
    <meta name="author" content="Sixun Dong">
    
    <!-- Open Graph Meta Tags -->
    <meta property="og:title" content="Sixun Dong - Academic Homepage">
    <meta property="og:description" content="PhD Student in Computer Science at Arizona State University, focusing on multimodal AI systems, computer vision, and machine learning">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://cv.ironieser.cc">
    <meta property="og:image" content="https://cv.ironieser.cc/images/logo.jpg">
    
    <!-- Twitter Card Meta Tags -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Sixun Dong - Academic Homepage">
    <meta name="twitter:description" content="PhD Student in Computer Science at Arizona State University, focusing on multimodal AI systems, computer vision, and machine learning">
    <meta name="twitter:image" content="https://cv.ironieser.cc/images/logo.jpg">
    
    <!-- JSON-LD Structured Data -->
    <script type="application/ld+json">
[
  {
    "@context": "https://schema.org",
    "@type": "Person",
    "name": "Sixun Dong",
    "alternateName": "Ironieser",
    "email": "ironieser@gmail.com",
    "jobTitle": "PhD Student in Computer Science",
    "affiliation": {
      "@type": "Organization",
      "name": "Arizona State University",
      "url": "https://www.asu.edu",
      "address": {
        "@type": "PostalAddress",
        "addressCountry": "USA",
        "addressLocality": "Tempe",
        "addressRegion": "AZ"
      }
    },
    "url": "https://cv.ironieser.cc",
    "sameAs": [
      "https://scholar.google.com/citations?user=j71Y2-4AAAAJ",
      "https://github.com/Ironieser",
      "https://twitter.com/ironieser",
      "https://cv.ironieser.cc",
      "https://ironieser.github.io"
    ],
    "knowsAbout": [
      "multimodal AI systems",
      "computer vision",
      "natural language processing",
      "machine learning"
    ],
    "image": "https://cv.ironieser.cc/images/logo.jpg"
  },
  {
    "@context": "https://schema.org",
    "@type": "WebSite",
    "name": "Sixun Dong (Ironieser) - Academic Homepage",
    "description": "PhD Student in Computer Science at Arizona State University, focusing on multimodal AI systems, computer vision, and machine learning",
    "url": "https://cv.ironieser.cc",
    "author": {
      "@type": "Person",
      "name": "Sixun Dong"
    },
    "publisher": {
      "@type": "Organization",
      "name": "Arizona State University",
      "url": "https://www.asu.edu"
    }
  },
  {
    "@type": "ScholarlyArticle",
    "name": "TransRAC: Encoding Multi-scale Temporal Correlation with Transformers for Repetitive Action Counting",
    "author": [
      {
        "@type": "Person",
        "name": "Huazhang Hu*"
      },
      {
        "@type": "Person",
        "name": "Sixun Dong*"
      },
      {
        "@type": "Person",
        "name": "Yiqun Zhao"
      },
      {
        "@type": "Person",
        "name": "Dongze Lian"
      },
      {
        "@type": "Person",
        "name": "Zhengxin Li"
      },
      {
        "@type": "Person",
        "name": "Shenghua Gao"
      }
    ],
    "datePublished": "2022-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CVPR'22"
    },
    "url": "https://arxiv.org/abs/2204.01018"
  },
  {
    "@type": "ScholarlyArticle",
    "name": "Weakly Supervised Video Representation Learning with Unaligned Text for Sequential Videos",
    "author": [
      {
        "@type": "Person",
        "name": "Sixun Dong*"
      },
      {
        "@type": "Person",
        "name": "Huazhang Hu*"
      },
      {
        "@type": "Person",
        "name": "Dongze Lian"
      },
      {
        "@type": "Person",
        "name": "Weixin Luo"
      },
      {
        "@type": "Person",
        "name": "Yicheng Qian"
      },
      {
        "@type": "Person",
        "name": "Shenghua Gao"
      }
    ],
    "datePublished": "2023-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CVPR'23"
    },
    "url": "https://arxiv.org/abs/2303.12370"
  },
  {
    "@type": "ScholarlyArticle",
    "name": "MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning",
    "author": [
      {
        "@type": "Person",
        "name": "Chenyu Wang"
      },
      {
        "@type": "Person",
        "name": "Weixin Luo"
      },
      {
        "@type": "Person",
        "name": "Sixun Dong"
      },
      {
        "@type": "Person",
        "name": "Xiaohua Xuan"
      },
      {
        "@type": "Person",
        "name": "Zhengxin Li"
      },
      {
        "@type": "Person",
        "name": "Lin Ma"
      },
      {
        "@type": "Person",
        "name": "Shenghua Gao"
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "WACV'25"
    },
    "url": "https://arxiv.org/abs/2401.10727"
  },
  {
    "@type": "ScholarlyArticle",
    "name": "RoomDesigner: Encoding Anchor-latents for Style-consistent and Shape-compatible Indoor Scene Generation",
    "author": [
      {
        "@type": "Person",
        "name": "Yiqun Zhao"
      },
      {
        "@type": "Person",
        "name": "Zibo Zhao"
      },
      {
        "@type": "Person",
        "name": "Jing Li"
      },
      {
        "@type": "Person",
        "name": "Sixun Dong"
      },
      {
        "@type": "Person",
        "name": "Shenghua Gao"
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "3DV'24"
    },
    "url": "https://arxiv.org/abs/2310.10027"
  },
  {
    "@type": "ScholarlyArticle",
    "name": "Sculpting Features from Noise: Reward-Guided Hierarchical Diffusion for Task-Optimal Feature Transformation",
    "author": [
      {
        "@type": "Person",
        "name": "Nanxu Gong*"
      },
      {
        "@type": "Person",
        "name": "Zijun Li*"
      },
      {
        "@type": "Person",
        "name": "Sixun Dong"
      },
      {
        "@type": "Person",
        "name": "Haoyue Bai"
      },
      {
        "@type": "Person",
        "name": "Wangyang Ying"
      },
      {
        "@type": "Person",
        "name": "Xinyuan Wang"
      },
      {
        "@type": "Person",
        "name": "Yanjie Fu"
      }
    ],
    "datePublished": "2025-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "NeurIPS'25"
    },
    "url": "https://arxiv.org/abs/2505.15152"
  },
  {
    "@type": "ScholarlyArticle",
    "name": "Unsupervised feature transformation via in-context generation, generator-critic llm agents, and duet-play teaming",
    "author": [
      {
        "@type": "Person",
        "name": "Nanxu Gong"
      },
      {
        "@type": "Person",
        "name": "Xinyuan Wang"
      },
      {
        "@type": "Person",
        "name": "Wangyang Ying"
      },
      {
        "@type": "Person",
        "name": "Haoyue Bai"
      },
      {
        "@type": "Person",
        "name": "Sixun Dong"
      },
      {
        "@type": "Person",
        "name": "Haifeng Chen"
      },
      {
        "@type": "Person",
        "name": "Yanjie Fu"
      }
    ],
    "datePublished": "2025-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "IJCAI'25"
    },
    "url": "https://arxiv.org/abs/2504.21304"
  }
]
</script>
    
    <!-- Favicon -->
    <link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
    <link rel="apple-touch-icon" sizes="180x180" href="apple-touch-icon.png">
    <link rel="shortcut icon" href="favicon.ico">
    
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <script src="script.js" defer></script>
</head>
<body>
    <!-- Navigation -->
    <header class="header">
        <nav class="nav">
            <div class="nav-container">
                <a href="index.html" class="nav-link active" >Bio</a>
                <a href="publications.html" class="nav-link " >Publications</a>
                <a href="blog.html" class="nav-link " >Blog</a>
                <a href="#" class="nav-link " target="_blank">CV(PDF)</a>
            </div>
        </nav>
    </header>

    <!-- Main Content -->
    <main class="main">
        <!-- Hero Section -->
        <section class="hero">
            <div class="container">
                <div class="hero-content">
                    <!-- Left: Photo -->
                    <div class="hero-photo">
                        <img src="images/logo.jpg" alt="Sixun Dong" class="profile-image">
                    </div>
                    
                    <!-- Right: Introduction -->
                    <div class="hero-info">
                        <h1 class="hero-title">Sixun Dong<span class="aka"> (Ironieser)</span></h1>
                        <p class="hero-subtitle">Multimodal Learning, VLM, LLM Agent</p>
                        <p class="hero-affiliation">Independent Researcher</p>
                        
                        <div class="hero-description">
                            <p>Currently, I am an independent researcher. I completed my Master's at ShanghaiTech University under Professor <a href="https://scholar.google.com/citations?hl=zh-CN&user=fe-1v0MAAAAJ" class="link">Shenghua Gao</a>.</p>
                            <p>My research focuses on <em class="research-keywords">multimodal AI systems</em> that bridge computer vision, natural language processing, and machine learning, with different applications.</p>
                        </div>
                        
                        <div class="hero-links">
                            
            <a href="mailto:sixundong.ai@gmail.com" class="hero-link" title="Email">
                <i class="fas fa-envelope"></i> Email
            </a>
            <a href="https://scholar.google.com/citations?user=j71Y2-4AAAAJ" class="hero-link" title="Scholar">
                <i class="fas fa-graduation-cap"></i> Scholar
            </a>
            <a href="https://github.com/Ironieser" class="hero-link" title="GitHub">
                <i class="fab fa-github"></i> GitHub
            </a>
            <a href="https://twitter.com/ironieser" class="hero-link" title="Twitter">
                <i class="fab fa-twitter"></i> Twitter
            </a>
            <a href="https://www.linkedin.com/in/sixun-dong-789069330/" class="hero-link" title="LinkedIn">
                <i class="fab fa-linkedin"></i> LinkedIn
            </a>
            <a href="https://zhuanlan.zhihu.com/p/1952615273212384694" class="hero-link" title="Áü•‰πé">
                <i class="fas fa-book"></i> Áü•‰πé
            </a>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- Recent News Section -->
        <section class="section-alt">
            <div class="container">
                <h2 class="section-title">Recent News</h2>
                
                <div class="news-container">
                    <div class="news-sidebar">
                        <button class="filter-btn active" data-filter="all">All</button>
                        <button class="filter-btn" data-filter="papers">üìÑ Papers</button>
                        <button class="filter-btn" data-filter="career">üíº Career</button>
                        <button class="filter-btn" data-filter="projects">üöÄ Projects</button>
                    </div>
                    
                    <div class="news-list">
                        
            <div class="news-item" data-category="papers">
                <span class="news-date">Sep 2025</span>
                <span class="news-content">Paper on <strong>Feature Transformation</strong> by Semi-AR and reward-guided diffusion, accepted to NeurIPS 2025, congratulations to Nanxu Gong and Zijun Li!</span>
            </div>
            <div class="news-item" data-category="career">
                <span class="news-date">Aug 2025</span>
                <span class="news-content">Completed GenAI Research Internship at <strong>Zoom Inc.</strong>, focusing on efficient vision‚Äìlanguage modeling. Grateful to my mentor <strong>Qi Qian</strong> and the Zoom team for their invaluable support and collaboration.</span>
            </div>
            <div class="news-item" data-category="papers">
                <span class="news-date">Aug 2025</span>
                <span class="news-content">Paper on <strong>MMTok</strong> - Multimodal Coverage Maximization for Efficient VLM Inference, project homepage launched</span>
            </div>
            <div class="news-item" data-category="papers">
                <span class="news-date">Aug 2025</span>
                <span class="news-content">Paper on <strong>LiveMCP-101</strong> - a new benchmark testing AI agents‚Äô real-world tool-use, released on arXiv</span>
            </div>
            <div class="news-item" data-category="papers">
                <span class="news-date">Aug 2025</span>
                <span class="news-content">Paper on <strong>LogicIF</strong> - Complex Logical Instruction Generation released on arXiv</span>
            </div>
            <div class="news-item" data-category="papers">
                <span class="news-date">Aug 2025</span>
                <span class="news-content">Published comprehensive blog post about <strong>TimesCLIP</strong> - our multimodal approach to time series forecasting with CLIP</span>
            </div>
            <div class="news-item" data-category="career">
                <span class="news-date">May 2025</span>
                <span class="news-content">Started GenAI Research Internship at <strong>Zoom Inc.</strong> focusing on efficient vision-language modeling</span>
            </div>
            <div class="news-item" data-category="papers">
                <span class="news-date">Feb 2024</span>
                <span class="news-content">Paper on <strong>MLLM-Tool</strong> accepted to WACV 2024</span>
            </div>
            <div class="news-item" data-category="papers">
                <span class="news-date">Mar 2023</span>
                <span class="news-content">Paper on <strong>WeakSVR</strong> accepted to CVPR 2023</span>
            </div>
            <div class="news-item" data-category="papers">
                <span class="news-date">Mar 2022</span>
                <span class="news-content">Paper on <strong>TransRAC</strong> accepted as <strong>üèÜ oral presentation</strong> to CVPR 2022</span>
            </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- Selected Publications -->
        <section class="section section-alt">
            <div class="container">
                <h2 class="section-title">Selected Publications</h2>
                <div class="publications-list">
                    
            <div class="publication-item">
                <img src="images/blog/mmtok/combined_plots.png" alt="MMTok: Multimodal Coverage Maximization for Efficient Inference of VLMs" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                <div class="publication-content">
                    <p class="publication-title"><span class="publication-venue-preprint">arXiv'2508</span> MMTok: Multimodal Coverage Maximization for Efficient Inference of VLMs</p>
                    <p class="publication-authors"><span class="author-highlight">Sixun Dong</span>, Juhua Hu, Mian Zhang, Ming Yin, Yanjie Fu, Qi Qian</p>
                    <p class="publication-links"><i class="ai ai-arxiv"></i> <a href="https://arxiv.org/abs/2508.18264" target="_blank">Paper</a> / <i class="fab fa-github"></i> <a href="https://github.com/Ironieser/MMTok" target="_blank">Code</a> / <i class="fas fa-home"></i> <a href="https://project.ironieser.cc/mmtok" target="_blank">Homepage</a> / <i class="fas fa-book-open"></i> <a href="https://zhuanlan.zhihu.com/p/1944299907109348831" target="_blank">Áü•‰πé</a></p>
                </div>
            </div>
            <div class="publication-item">
                <img src="teaser/MCP101.jpg" alt="LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on Challenging Queries" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                <div class="publication-content">
                    <p class="publication-title"><span class="publication-venue-preprint">arXiv'2508</span> LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on Challenging Queries</p>
                    <p class="publication-authors">Ming Yin, Dinghan Shen, Silei Xu, Jianbing Han, <span class="author-highlight">Sixun Dong</span>, Mian Zhang, Yebowen Hu, Shujian Liu, Simin Ma, Song Wang, Sathish Reddy Indurthi, Xun Wang, Yiran Chen, Kaiqiang Song</p>
                    <p class="publication-links"><i class="ai ai-arxiv"></i> <a href="https://arxiv.org/abs/2508.15760" target="_blank">Paper</a> / <i class="fab fa-github"></i> <span class="coming-soon">Code</span> / <i class="fas fa-database"></i> <span class="coming-soon">Benchmark</span> / <i class="fas fa-book-open"></i> <a href="https://zhuanlan.zhihu.com/p/1949009535743264421" target="_blank">Áü•‰πé</a></p>
                </div>
            </div>
            <div class="publication-item">
                <img src="teaser/logitif.jpg" alt="Complex Logical Instruction Generation" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                <div class="publication-content">
                    <p class="publication-title"><span class="publication-venue-preprint">arXiv'2508</span> Complex Logical Instruction Generation</p>
                    <p class="publication-authors">Mian Zhang, Shujian Liu, <span class="author-highlight">Sixun Dong</span>, Ming Yin, Yebowen Hu, Xun Wang, Steven Ma, Song Wang, Sathish Reddy Indurthi, Haoyun Deng, Zhiyu Zoey Chen, Kaiqiang Song</p>
                    <p class="publication-links"><i class="ai ai-arxiv"></i> <a href="https://arxiv.org/abs/2508.09125" target="_blank">Paper</a> / <i class="fab fa-github"></i> <a href="https://github.com/mianzhang/LogicIF" target="_blank">Code</a> / <i class="fas fa-database"></i> <a href="https://github.com/mianzhang/LogicIF" target="_blank">Benchmark</a> / <i class="fas fa-book-open"></i> <a href="https://zhuanlan.zhihu.com/p/1941376510113059206" target="_blank">Áü•‰πé</a></p>
                </div>
            </div>
            <div class="publication-item">
                <img src="images/blog/timesclip/fig3.png" alt="Teaching Time Series to See and Speak: Forecasting with Aligned Visual and Textual Perspectives" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                <div class="publication-content">
                    <p class="publication-title"><span class="publication-venue-under-review">arXiv'2506</span> Teaching Time Series to See and Speak: Forecasting with Aligned Visual and Textual Perspectives</p>
                    <p class="publication-authors"><span class="author-highlight">Sixun Dong</span>, Wei Fan, Teresa Wu, Yanjie Fu</p>
                    <p class="publication-links"><i class="ai ai-arxiv"></i> <a href="https://arxiv.org/abs/2506.24124" target="_blank">Paper</a> / <i class="fab fa-github"></i> <a href="https://github.com/Ironieser/TimesCLIP" target="_blank">Code</a> / <i class="fas fa-home"></i> <a href="https://cv.ironieser.cc/projects/timesclip.html" target="_blank">Homepage</a> / <i class="fas fa-blog"></i> <a href="https://cv.ironieser.cc/blog.html#timesclip-time-series-forecasting" target="_blank">Blog</a> / <i class="fas fa-book-open"></i> <a href="https://zhuanlan.zhihu.com/p/1936813469446940130" target="_blank">Áü•‰πé</a></p>
                </div>
            </div>
            <div class="publication-item">
                <img src="teaser/sculp.jpg" alt="Sculpting Features from Noise: Reward-Guided Hierarchical Diffusion for Task-Optimal Feature Transformation" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                <div class="publication-content">
                    <p class="publication-title"><span class="publication-venue">NeurIPS'25</span> Sculpting Features from Noise: Reward-Guided Hierarchical Diffusion for Task-Optimal Feature Transformation</p>
                    <p class="publication-authors">Nanxu Gong*, Zijun Li*, <span class="author-highlight">Sixun Dong</span>, Haoyue Bai, Wangyang Ying, Xinyuan Wang, Yanjie Fu</p>
                    <p class="publication-links"><i class="ai ai-arxiv"></i> <a href="https://arxiv.org/abs/2505.15152" target="_blank">Paper</a> / <i class="fab fa-github"></i> <a href="https://github.com/NanxuGong/DIFFT" target="_blank">Code</a> / <i class="fas fa-book-open"></i> <a href="https://zhuanlan.zhihu.com/p/1933301751826429982" target="_blank">Áü•‰πé</a></p>
                </div>
            </div>
            <div class="publication-item">
                <img src="teaser/mmtool.jpg" alt="MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                <div class="publication-content">
                    <p class="publication-title"><span class="publication-venue">WACV'25</span> MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning</p>
                    <p class="publication-authors">Chenyu Wang, Weixin Luo, <span class="author-highlight">Sixun Dong</span>, Xiaohua Xuan, Zhengxin Li, Lin Ma, Shenghua Gao</p>
                    <p class="publication-links"><i class="ai ai-arxiv"></i> <a href="https://arxiv.org/abs/2401.10727" target="_blank">Paper</a> / <i class="fab fa-github"></i> <a href="https://github.com/MLLM-Tool/MLLM-Tool" target="_blank">Code</a></p>
                </div>
            </div>
            <div class="publication-item">
                <img src="teaser/room.jpg" alt="RoomDesigner: Encoding Anchor-latents for Style-consistent and Shape-compatible Indoor Scene Generation" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                <div class="publication-content">
                    <p class="publication-title"><span class="publication-venue">3DV'24</span> RoomDesigner: Encoding Anchor-latents for Style-consistent and Shape-compatible Indoor Scene Generation</p>
                    <p class="publication-authors">Yiqun Zhao, Zibo Zhao, Jing Li, <span class="author-highlight">Sixun Dong</span>, Shenghua Gao</p>
                    <p class="publication-links"><i class="ai ai-arxiv"></i> <a href="https://arxiv.org/abs/2310.10027" target="_blank">Paper</a> / <i class="fab fa-github"></i> <a href="https://github.com/zhao-yiqun/RoomDesigner" target="_blank">Code</a></p>
                </div>
            </div>
            <div class="publication-item">
                <img src="teaser/weaksvr.jpg" alt="Weakly Supervised Video Representation Learning with Unaligned Text for Sequential Videos" class="publication-image teaser" onerror="this.src='images/default-paper.png'">
                <div class="publication-content">
                    <p class="publication-title"><span class="publication-venue">CVPR'23</span> Weakly Supervised Video Representation Learning with Unaligned Text for Sequential Videos</p>
                    <p class="publication-authors"><span class="author-highlight">Sixun Dong*</span>, Huazhang Hu*, Dongze Lian, Weixin Luo, Yicheng Qian, Shenghua Gao</p>
                    <p class="publication-links"><i class="ai ai-arxiv"></i> <a href="https://arxiv.org/abs/2303.12370" target="_blank">Paper</a> / <i class="fab fa-github"></i> <a href="https://github.com/svip-lab/WeakSVR" target="_blank">Code</a> / <i class="fab fa-youtube"></i> <a href="https://www.youtube.com/watch?v=AqozSRYP7Pc" target="_blank">YouTube</a> / <i class="fas fa-tv"></i> <a href="https://www.bilibili.com/video/BV1AW4y1R7um" target="_blank">Bilibili</a> / <i class="fas fa-book-open"></i> <a href="https://zhuanlan.zhihu.com/p/617926257" target="_blank">Áü•‰πé</a></p>
                </div>
            </div>
                </div>
                
                <div class="section-footer">
                    <a href="publications.html" class="btn btn-more">View All Publications</a>
                </div>
            </div>
        </section>

        <!-- Experience -->
        <section class="section section-alt">
            <div class="container">
                <h2 class="section-title">Experience</h2>
                <div class="experience-list">
                    
            <div class="experience-item">
                <img src="images/zoom.jpg" alt="Zoom Inc., GenAI Research Group" class="experience-logo">
                <div class="experience-content">
                    <p class="experience-position">GenAI Research Intern</p>
                    <p class="experience-company">Zoom Inc., GenAI Research Group</p>
                    <p class="experience-period">May 2025 - Aug 2025</p>
                    <p class="experience-description">Worked on VLM and LLM Agent. Published one first-author paper on efficient VLM inference and two collaborative papers on LLM evaluation.</p>
                </div>
            </div>
            <div class="experience-item">
                <img src="images/dgene.jpeg" alt="DGene, Digital Human Algorithm Department" class="experience-logo">
                <div class="experience-content">
                    <p class="experience-position">Research Intern (Team Leader)</p>
                    <p class="experience-company">DGene, Digital Human Algorithm Department</p>
                    <p class="experience-period">Aug 2023 - Jan 2024</p>
                    <p class="experience-description">Led digital human projects: co-speech gesture generation and 3D human body reconstruction with <7% measurement error.</p>
                </div>
            </div>
            <div class="experience-item">
                <img src="images/transsion.jpeg" alt="Transsion Holdings, Audio-Video Generation Department" class="experience-logo">
                <div class="experience-content">
                    <p class="experience-position">Research Intern (Team Leader)</p>
                    <p class="experience-company">Transsion Holdings, Audio-Video Generation Department</p>
                    <p class="experience-period">Apr 2023 - Aug 2023</p>
                    <p class="experience-description">Led audio-driven talking head video generation research, achieving SoTA performance in commercial and academic benchmarks.</p>
                </div>
            </div>
                </div>
            </div>
        </section>

        <!-- Academic Service -->
        <section class="section section-alt">
            <div class="container">
                <h2 class="section-title">Academic Service</h2>
                <div class="service-content">
                    <div class="service-summary">
                        <h3 class="service-title">Reviewer</h3>
                        <p class="service-description">
                            <strong>Conferences:</strong> CVPR 2023+, ICCV 2023+, ECCV 2024+, NeurIPS 2025+, ICLR 2026+, ACCV 2024, ACM MM 2023-2025, KDD 2024
                        </p>
                        <p class="service-description">
                            <strong>Journals:</strong> TMM, Neural Networks, TKDD
                        </p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Education -->
        <section class="section section-alt">
            <div class="container">
                <h2 class="section-title">Education</h2>
                <div class="education-list">
                    
            <div class="education-item">
                <span class="education-period">2021 - 2024</span>
                <div class="education-content">
                    <p class="education-degree">M.S. in Computer Science</p>
                    <p class="education-institution">ShanghaiTech University, China</p>
                    <p class="education-details">SVIP-Lab, Advisor: Prof. Shenghua Gao</p>
                </div>
            </div>
            <div class="education-item">
                <span class="education-period">2016 - 2020</span>
                <div class="education-content">
                    <p class="education-degree">B.E. in Computer Science (Dual Degree)</p>
                    <p class="education-institution">Dalian University of Technology, China</p>
                    
                </div>
            </div>
            <div class="education-item">
                <span class="education-period">2016 - 2020</span>
                <div class="education-content">
                    <p class="education-degree">B.E. in Process Equipment and Control Engineering</p>
                    <p class="education-institution">Dalian University of Technology, China</p>
                    
                </div>
            </div>
                </div>
            </div>
        </section>
    </main>

    
    <footer class="footer">
        <div class="container">
            <!-- Visitor Map Section -->
            <div class="visitor-map-section">
                <div class="visitor-map-container">
                    <!-- Visitor Map Widget -->
                    <div class="visitor-map">
                        <!-- ClustrMaps Widget -->
                        <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=r_cMMykDPAdqK2GTahWbR__mtnzcj9svUgejZ86OXnU&cl=ffffff&w=a"></script>
                    </div>
                </div>
            </div>

            <div class="footer-stats">
                <div class="stats-item">
                    <i class="fas fa-map-marker-alt"></i>
                    Last updated from: <span id="owner-location">Loading...</span>
                </div>
                <div class="stats-item">
                    <i class="fas fa-clock"></i>
                    Content last updated: <span id="last-updated"></span>
                </div>
            </div>
            
            <div class="template-credit">
                <p>Built with <a href="https://github.com/Ironieser/ironieser.github.io" target="_blank" rel="noopener">Config-Driven Academic Website Template</a> by <a href="https://github.com/Ironieser/ironieser.github.io" target="_blank" rel="noopener">Sixun Dong (ironieser)</a></p>
                <p class="template-acknowledgments">Built with the help of Cursor IDE, Claude AI, and GPT - enabling non-experts to create personalized academic websites through modern AI tools</p>
            </div>
            <p>&copy; 2025 Sixun Dong. All rights reserved.</p>
        </div>
    </footer>
    
    <script>
        // News filter functionality
        function initNewsFilter() {
            const filterBtns = document.querySelectorAll('.filter-btn');
            const newsItems = document.querySelectorAll('.news-item');
            const categoryIndicators = document.querySelectorAll('.category-indicator');
            
            filterBtns.forEach(btn => {
                btn.addEventListener('click', function() {
                    const filter = this.getAttribute('data-filter');
                    
                    // Update active button
                    filterBtns.forEach(b => b.classList.remove('active'));
                    this.classList.add('active');
                    
                    // Update active category indicator
                    categoryIndicators.forEach(indicator => {
                        indicator.classList.remove('active');
                        if (indicator.getAttribute('data-category') === filter) {
                            indicator.classList.add('active');
                        }
                    });
                    
                    // Filter news items
                    newsItems.forEach(item => {
                        if (filter === 'all' || item.getAttribute('data-category') === filter) {
                            item.style.display = 'block';
                        } else {
                            item.style.display = 'none';
                        }
                    });
                });
            });
        }
        
        // Initialize news filter on page load
        document.addEventListener('DOMContentLoaded', function() {
            initNewsFilter();
        });
    </script>
    
    
    <script>
        // Get website owner's location during build (not visitor's location)
        async function getOwnerLocation() {
            try {
                // Try primary API first
                let response = await fetch('https://ipapi.co/json/');
                let data = await response.json();
                
                if (data.city && data.country_name) {
                    const location = `${data.city}, ${data.country_name}`;
                    document.getElementById('owner-location').textContent = location;
                    return;
                }
                
                // If primary API fails, try backup API
                response = await fetch('https://api.ipify.org?format=json');
                const ipData = await response.json();
                
                if (ipData.ip) {
                    // Use a different geolocation service
                    response = await fetch(`https://ip-api.com/json/${ipData.ip}`);
                    data = await response.json();
                    
                    if (data.city && data.country) {
                        const location = `${data.city}, ${data.country}`
                        document.getElementById('owner-location').textContent = location;
                        return;
                    }
                }
                
                // If all APIs fail, show a default message
                document.getElementById('owner-location').textContent = 'Remote Server';
                
            } catch (error) {
                console.log('Location detection failed:', error);
                // For GitHub Actions builds, show a more appropriate message
                document.getElementById('owner-location').textContent = 'GitHub Actions';
            }
        }
        
        // Set last updated time (when GitHub Actions built the site)
        function setLastUpdated() {
            const buildDate = '2025-10-12';
            const date = new Date(buildDate + 'T12:00:00'); // Add noon time to avoid timezone issues
            const formatted = date.toLocaleDateString('en-US', { 
                year: 'numeric', 
                month: 'short', 
                day: 'numeric' 
            });
            document.getElementById('last-updated').textContent = formatted;
        }
        
        // Initialize on page load
        document.addEventListener('DOMContentLoaded', function() {
            getOwnerLocation();
            setLastUpdated();
        });
    </script>
</body>
</html>