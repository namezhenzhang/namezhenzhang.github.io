#!/usr/bin/env python3
"""
Google Scholar Publications Sync - Pythonç‰ˆæœ¬
åŠŸèƒ½ï¼šä»Google Scholarè‡ªåŠ¨è·å–å‡ºç‰ˆç‰©å¹¶æ›´æ–°config.json
"""

import json
import re
import requests
from bs4 import BeautifulSoup
import time
import os
import sys
from datetime import datetime
from urllib.parse import urljoin, urlparse
import argparse

# é…ç½®
SCHOLAR_USER_ID = "j71Y2-4AAAAJ"  # æ‚¨çš„Scholar ID
REQUEST_DELAY = 2  # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
MAX_RETRIES = 3

class ScholarSync:
    def __init__(self, user_id=None, config_path="config.json"):
        self.user_id = user_id or SCHOLAR_USER_ID
        self.config_path = config_path
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    def fetch_scholar_publications(self):
        """ä»Google Scholarè·å–å‡ºç‰ˆç‰©åˆ—è¡¨"""
        print(f"ğŸ” Fetching publications from Google Scholar...")
        
        url = f"https://scholar.google.com/citations?user={self.user_id}&hl=en&sortby=pubdate"
        print(f"ğŸ“– URL: {url}")
        
        for attempt in range(MAX_RETRIES):
            try:
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æŸ¥æ‰¾å‡ºç‰ˆç‰©è¡¨æ ¼
                pub_table = soup.find('table', id='gsc_a_t')
                if not pub_table:
                    print("âŒ Could not find publications table")
                    return []
                
                publications = []
                rows = pub_table.find_all('tr', class_='gsc_a_tr')
                
                for row in rows:
                    try:
                        pub = self._parse_publication_row(row)
                        if pub:
                            publications.append(pub)
                    except Exception as e:
                        print(f"âš ï¸  Error parsing row: {e}")
                        continue
                
                print(f"ğŸ“š Found {len(publications)} publications")
                return publications
                
            except requests.RequestException as e:
                print(f"âŒ Request failed (attempt {attempt + 1}/{MAX_RETRIES}): {e}")
                if attempt < MAX_RETRIES - 1:
                    time.sleep(REQUEST_DELAY * (attempt + 1))
                    continue
                else:
                    return []
        
        return []
    
    def _parse_publication_row(self, row):
        """è§£æå•ä¸ªå‡ºç‰ˆç‰©è¡Œ"""
        # æ ‡é¢˜å’Œé“¾æ¥
        title_elem = row.find('a', class_='gsc_a_at')
        if not title_elem:
            return None
        
        title = title_elem.get_text().strip()
        link = title_elem.get('href', '')
        if link:
            link = urljoin("https://scholar.google.com", link)
        
        # ä½œè€…
        authors_elem = row.find('div', class_='gs_gray')
        authors = []
        if authors_elem:
            authors_text = authors_elem.get_text().strip()
            authors = [a.strip() for a in authors_text.split(',')]
        
        # ä¼šè®®/æœŸåˆŠ
        venue_elems = row.find_all('div', class_='gs_gray')
        venue = ""
        if len(venue_elems) > 1:
            venue = venue_elems[1].get_text().strip()
        
        # å¹´ä»½
        year_elem = row.find('span', class_='gsc_a_h')
        year = 2024  # é»˜è®¤å¹´ä»½
        if year_elem:
            year_text = year_elem.get_text().strip()
            if year_text.isdigit():
                year = int(year_text)
        
        # å¼•ç”¨æ•°
        cite_elem = row.find('a', class_='gsc_a_c')
        citations = 0
        if cite_elem:
            cite_text = cite_elem.get_text().strip()
            if cite_text.isdigit():
                citations = int(cite_text)
        
        return {
            'title': title,
            'authors': authors,
            'venue': venue,
            'year': year,
            'citations': citations,
            'link': link
        }
    
    def parse_venue_info(self, venue):
        """è§£æä¼šè®®/æœŸåˆŠä¿¡æ¯"""
        venue_map = {
            'CVPR': {'type': 'conference', 'fullName': 'CVPR'},
            'ICCV': {'type': 'conference', 'fullName': 'ICCV'},
            'ECCV': {'type': 'conference', 'fullName': 'ECCV'},
            'NeurIPS': {'type': 'conference', 'fullName': 'NeurIPS'},
            'ICML': {'type': 'conference', 'fullName': 'ICML'},
            'ICLR': {'type': 'conference', 'fullName': 'ICLR'},
            'AAAI': {'type': 'conference', 'fullName': 'AAAI'},
            'IJCAI': {'type': 'conference', 'fullName': 'IJCAI'},
            'WACV': {'type': 'conference', 'fullName': 'WACV'},
            '3DV': {'type': 'conference', 'fullName': '3DV'},
            'arXiv': {'type': 'preprint', 'fullName': 'arXiv'}
        }
        
        venue_lower = venue.lower()
        
        # æ£€æŸ¥å·²çŸ¥ä¼šè®®/æœŸåˆŠ
        for key, value in venue_map.items():
            if key.lower() in venue_lower:
                return value
        
        # é»˜è®¤å¤„ç†
        if 'arxiv' in venue_lower:
            return {'type': 'preprint', 'fullName': 'arXiv'}
        elif 'journal' in venue_lower:
            return {'type': 'journal', 'fullName': venue}
        else:
            return {'type': 'conference', 'fullName': venue}
    
    def extract_arxiv_info(self, venue):
        """ä»arXivä¿¡æ¯ä¸­æå–å¹´ä»½å’Œæœˆä»½ä¿¡æ¯"""
        if not venue or 'arxiv' not in venue.lower():
            return None, None
        
        # åŒ¹é…æ ¼å¼ï¼šarXiv:YYMM.NNNNN, arxiv.org/abs/YYMM.NNNNN, arXiv YYMM.NNNNN
        arxiv_match = re.search(r'arxiv[:\s/]*(?:abs/)?(\d{4})\.', venue.lower())
        if arxiv_match:
            yymm = arxiv_match.group(1)
            year_part = int(yymm[:2])
            month_part = yymm[2:]
            
            # ç›´æ¥ä½¿ç”¨arXiv IDä¸­çš„å¹´ä»½
            # arXivæ ¼å¼ï¼šYYMM.NNNNNï¼Œå…¶ä¸­YYæ˜¯å¹´ä»½çš„åä¸¤ä½ï¼ŒMMæ˜¯æœˆä»½
            if year_part >= 7:  # 07-99 è¡¨ç¤º 2007-2099
                year = 2000 + year_part
            else:  # 00-06 è¡¨ç¤º 2000-2006
                year = 2000 + year_part
            
            return year, yymm  # è¿”å›å®Œæ•´å¹´ä»½å’ŒYYMM
        
        return None, None
    
    def smart_year_detection(self, pub, venue_info):
        """æ™ºèƒ½å¹´ä»½æ£€æµ‹"""
        current_year = datetime.now().year
        
        # 1. å°è¯•ä»arXivä¿¡æ¯æå–å¹´ä»½
        arxiv_year, arxiv_yymm = self.extract_arxiv_info(pub['venue'])
        
        # 2. å°è¯•ä»venueä¿¡æ¯ä¸­æå–å¹´ä»½
        venue_year = None
        if venue_info['fullName']:
            year_match = re.search(r'20\d{2}', venue_info['fullName'])
            if year_match:
                venue_year = int(year_match.group(0))
        
        # 3. Scholaræä¾›çš„å¹´ä»½
        scholar_year = pub['year']
        
        # 4. æ™ºèƒ½é€‰æ‹©ä¼˜å…ˆçº§ï¼šarXiv > venue > Scholar
        if arxiv_year and 2000 <= arxiv_year <= current_year + 1:  # å…è®¸æœªæ¥ä¸€å¹´
            print(f"ğŸ“… Using arXiv year {arxiv_year} for '{pub['title'][:50]}...'")
            return arxiv_year, arxiv_yymm
        elif venue_year and 2020 <= venue_year <= current_year + 1:
            print(f"ğŸ“… Using venue year {venue_year} for '{pub['title'][:50]}...'")
            return venue_year, None
        elif 2020 <= scholar_year <= current_year + 1:
            print(f"ğŸ“… Using Scholar year {scholar_year} for '{pub['title'][:50]}...'")
            return scholar_year, None
        else:
            print(f"âš ï¸  No reliable year found for '{pub['title'][:50]}...', using current year {current_year}")
            return current_year, None
    
    def check_duplicate_across_all_years(self, new_pub, existing_config):
        """è·¨å¹´ä»½æ™ºèƒ½é‡å¤æ£€æµ‹"""
        # æ”¶é›†æ‰€æœ‰å¹´ä»½çš„æ‰€æœ‰è®ºæ–‡
        all_existing_pubs = []
        for year, pubs in existing_config.get('publications', {}).items():
            if isinstance(pubs, list):
                all_existing_pubs.extend(pubs)
        
        for existing in all_existing_pubs:
            # 1. å®Œå…¨æ ‡é¢˜åŒ¹é…
            if existing['title'].lower() == new_pub['title'].lower():
                print(f"ğŸ” Found exact title match: \"{new_pub['title']}\"")
                return existing
            
            # 2. æ ‡é¢˜ç›¸ä¼¼åº¦æ£€æµ‹ï¼ˆå»é™¤æ ‡ç‚¹ç¬¦å·åæ¯”è¾ƒï¼‰
            clean_new = re.sub(r'[^a-z0-9\s]', '', new_pub['title'].lower()).strip()
            clean_existing = re.sub(r'[^a-z0-9\s]', '', existing['title'].lower()).strip()
            if clean_new == clean_existing:
                print(f"ğŸ” Found similar title: \"{new_pub['title']}\" vs \"{existing['title']}\"")
                return existing
            
            # 3. æ ¸å¿ƒè¯åŒ¹é…ï¼ˆæ ‡é¢˜é•¿åº¦>20å­—ç¬¦æ—¶ï¼‰
            if len(new_pub['title']) > 20 and len(existing['title']) > 20:
                new_words = [w for w in clean_new.split() if len(w) > 3]
                existing_words = [w for w in clean_existing.split() if len(w) > 3]
                common_words = [w for w in new_words if w in existing_words]
                
                # å¦‚æœ85%ä»¥ä¸Šçš„é‡è¦è¯æ±‡ç›¸åŒï¼Œè®¤ä¸ºæ˜¯é‡å¤
                if len(common_words) / max(len(new_words), len(existing_words)) > 0.85:
                    print(f"ğŸ” Found word overlap match: \"{new_pub['title']}\" vs \"{existing['title']}\"")
                    return existing
            
            # 4. ä½œè€…+å…³é”®è¯åŒ¹é…
            new_authors = [a.lower().strip() for a in new_pub['authors']]
            existing_authors = [a.lower().strip() for a in existing.get('authors', [])]
            
            common_authors = []
            for na in new_authors:
                for ea in existing_authors:
                    if len(na) > 2 and len(ea) > 2 and (na in ea or ea in na):
                        common_authors.append(na)
                        break
            
            # å¦‚æœæœ‰å…±åŒä½œè€…ä¸”æ ‡é¢˜æœ‰é‡å è¯æ±‡ï¼Œå¯èƒ½æ˜¯é‡å¤
            if len(common_authors) >= 3:  # è‡³å°‘3ä¸ªå…±åŒä½œè€…
                title_overlap = len([w for w in clean_new.split() 
                                   if len(w) > 4 and w in clean_existing])
                
                # å¯¹äºsurveyè®ºæ–‡ï¼Œéœ€è¦æ›´ä¸¥æ ¼çš„åŒ¹é…æ¡ä»¶
                is_survey_new = 'survey' in clean_new.lower()
                is_survey_existing = 'survey' in clean_existing.lower()
                
                if is_survey_new and is_survey_existing:
                    # Surveyè®ºæ–‡éœ€è¦æ›´é«˜çš„ç›¸ä¼¼åº¦é˜ˆå€¼
                    new_words = clean_new.split()
                    existing_words = clean_existing.split()
                    overlap_ratio = title_overlap / max(len(new_words), len(existing_words))
                    
                    if overlap_ratio >= 0.7:
                        print(f"ğŸ” Found survey paper match: \"{new_pub['title']}\" vs \"{existing['title']}\" (common authors: {len(common_authors)}, title overlap: {title_overlap}, ratio: {overlap_ratio:.2f})")
                        return existing
                    else:
                        print(f"ğŸ“ Survey papers with similar authors but different topics: \"{new_pub['title'][:50]}...\" vs \"{existing['title'][:50]}...\" (overlap ratio: {overlap_ratio:.2f})")
                else:
                    # ésurveyè®ºæ–‡ä½¿ç”¨åŸæ¥çš„æ¡ä»¶
                    if title_overlap >= 5:
                        print(f"ğŸ” Found author+title match: \"{new_pub['title']}\" vs \"{existing['title']}\" (common authors: {len(common_authors)}, title overlap: {title_overlap})")
                        return existing
        
        return None
    
    def is_venue_user_customized(self, venue):
        """æ£€æµ‹venueæ˜¯å¦æ˜¯ç”¨æˆ·æ‰‹åŠ¨è®¾ç½®çš„æ ¼å¼"""
        if not venue:
            return False
        
        import re
        
        # 1. åŒ…å«å¹´ä»½çš„æ ¼å¼ï¼ˆå¦‚ CVPR'22, WACV'25, NeurIPS'25, 3DV'24ï¼‰
        if re.match(r"^[A-Za-z0-9]+'[0-9]{2}$", venue):
            return True
        
        # 2. åŒ…å«å®Œæ•´å¹´ä»½çš„æ ¼å¼ï¼ˆå¦‚ CVPR 2022, WACV 2025ï¼‰
        if re.match(r"^[A-Za-z]+\s+20[0-9]{2}$", venue):
            return True
        
        # 3. åŒ…å«ç‰¹æ®Šæ ¼å¼çš„arXivï¼ˆå¦‚ arXiv'2508ï¼‰
        if re.match(r"^arXiv'[0-9]{4}$", venue):
            return True
        
        # 4. åŒ…å«ç‰¹æ®Šæ ‡è¯†çš„venueï¼ˆå¦‚ Under Review, Journalç­‰ï¼‰
        special_venues = ['Under Review', 'Journal', 'Conference']
        if venue in special_venues:
            return True
        
        # 5. åŒ…å«ç‰¹æ®Šå­—ç¬¦æˆ–æ ¼å¼çš„venue
        if any(char in venue for char in ['(', ')', '&', 'and']):
            return True
        
        return False

    def update_existing_publication(self, existing, scholar_data, formatted_venue=None):
        """æ›´æ–°å·²å­˜åœ¨çš„è®ºæ–‡ä¿¡æ¯"""
        updated = False
        
        # 1. æ›´æ–°å¼•ç”¨æ•°ç›¸å…³çš„featuredçŠ¶æ€ï¼ˆå¦‚æœç”¨æˆ·æ²¡æœ‰æ‰‹åŠ¨è®¾ç½®ï¼‰
        should_be_featured = scholar_data['citations'] > 10
        if should_be_featured and not existing.get('featured'):
            existing['featured'] = True
            updated = True
            print(f"  âœ¨ Marked as featured (citations: {scholar_data['citations']})")
        
        # 2. æ›´æ–°å¼•ç”¨æ•°ä¿¡æ¯ï¼ˆå¦‚æœé…ç½®ä¸­æ²¡æœ‰citationså­—æ®µï¼‰
        if 'citations' not in existing and scholar_data['citations'] > 0:
            existing['citations'] = scholar_data['citations']
            updated = True
            print(f"  ğŸ“Š Added citations count: {scholar_data['citations']}")
        
        # 3. ä¿æŠ¤ç”¨æˆ·æ‰‹åŠ¨è®¾ç½®çš„venueä¿¡æ¯
        is_user_customized_venue = self.is_venue_user_customized(existing.get('venue', ''))
        if not is_user_customized_venue:
            # æ›´æ–°venueæ ¼å¼ï¼ˆå¦‚æœæä¾›äº†æ–°çš„æ ¼å¼åŒ–venueä¸”å½“å‰æ˜¯ç®€å•çš„arXivæ ¼å¼ï¼‰
            if (formatted_venue and 
                existing.get('venue') == 'arXiv' and 
                formatted_venue.startswith("arXiv'") and
                existing.get('auto_sync') is not False):
                existing['venue'] = formatted_venue
                updated = True
                print(f"  ğŸ“ Updated venue format: {formatted_venue}")
        else:
            print(f"  ğŸ”’ Protected user-customized venue: \"{existing.get('venue')}\"")
        
        return updated
    
    def convert_to_config_format(self, scholar_pubs, existing_config):
        """å°†Scholaræ•°æ®è½¬æ¢ä¸ºconfigæ ¼å¼"""
        print("ğŸ”„ Converting Scholar data to config format...")
        
        publications_by_year = {}
        added_count = 0
        updated_count = 0
        
        for pub in scholar_pubs:
            venue_info = self.parse_venue_info(pub['venue'])
            smart_year, arxiv_yymm = self.smart_year_detection(pub, venue_info)
            year = str(smart_year)
            
            # æ ¹æ®arXivä¿¡æ¯æ ¼å¼åŒ–venue
            if arxiv_yymm and venue_info['type'] == 'preprint':
                formatted_venue = f"arXiv'{arxiv_yymm}"
            else:
                formatted_venue = venue_info['fullName']
            
            # è·¨å¹´ä»½æ™ºèƒ½é‡å¤æ£€æµ‹
            existing = self.check_duplicate_across_all_years(pub, existing_config)
            
            if not existing:
                # æ–°è®ºæ–‡ï¼šæ·»åŠ åŸºç¡€ä¿¡æ¯
                if year not in publications_by_year:
                    publications_by_year[year] = []
                
                config_pub = {
                    'title': pub['title'],
                    'authors': pub['authors'],
                    'venue': formatted_venue,
                    'venue_type': venue_info['type'],
                    'image': "teaser/preprint.jpg",  # è‡ªåŠ¨åŒæ­¥çš„è®ºæ–‡ç»Ÿä¸€ä½¿ç”¨preprintå›¾ç‰‡
                    'auto_sync': True,  # æ ‡è®°ä¸ºè‡ªåŠ¨åŒæ­¥
                    'links': [
                        {
                            'name': 'Paper',
                            'url': pub['link'] if pub['link'] else '#',
                            'icon': 'ai ai-arxiv'
                        }
                    ]
                }
                
                # å¦‚æœå¼•ç”¨æ•°è¾ƒé«˜ï¼Œæ ‡è®°ä¸ºfeatured
                if pub['citations'] > 10:
                    config_pub['featured'] = True
                
                publications_by_year[year].append(config_pub)
                print(f"âœ… Added new: {pub['title']} ({year}) - venue: {formatted_venue}")
                added_count += 1
            else:
                # å·²å­˜åœ¨çš„è®ºæ–‡ï¼šæ£€æŸ¥æ˜¯å¦å…è®¸è‡ªåŠ¨æ›´æ–°
                if existing.get('auto_sync') is False:
                    print(f"ğŸ”’ Protected: {existing['title']} (auto_sync disabled)")
                else:
                    was_updated = self.update_existing_publication(existing, pub, formatted_venue)
                    if was_updated:
                        print(f"ğŸ”„ Updated: {existing['title']} (citations: {pub['citations']})")
                        updated_count += 1
                    else:
                        print(f"â„¹ï¸  Skipped: {existing['title']} (no updates needed)")
        
        print(f"ğŸ“Š Summary: {added_count} new, {updated_count} updated")
        return publications_by_year, updated_count > 0
    
    def load_config(self):
        """åŠ è½½ç°æœ‰é…ç½®"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"âŒ Config file not found: {self.config_path}")
            return None
        except json.JSONDecodeError as e:
            print(f"âŒ Config file syntax error: {e}")
            return None
    
    def save_config(self, config_data):
        """ä¿å­˜é…ç½®"""
        try:
            with open(self.config_path, 'w', encoding='utf-8') as f:
                json.dump(config_data, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ Config saved to {self.config_path}")
            return True
        except Exception as e:
            print(f"âŒ Error saving config: {e}")
            return False
    
    def sync(self, dry_run=False):
        """æ‰§è¡ŒåŒæ­¥"""
        print("ğŸš€ Starting Google Scholar sync...")
        print(f"ğŸ‘¤ Scholar ID: {self.user_id}")
        print(f"ğŸ“„ Config file: {self.config_path}")
        
        # 1. åŠ è½½ç°æœ‰é…ç½®
        print("\nğŸ“– Loading current config...")
        config_data = self.load_config()
        if not config_data:
            return False
        
        # 2. è·å–Scholaræ•°æ®
        print("\nğŸ” Fetching from Google Scholar...")
        scholar_pubs = self.fetch_scholar_publications()
        if not scholar_pubs:
            print("âš ï¸  No publications found, skipping update")
            return False
        
        # 3. è½¬æ¢å’Œåˆå¹¶æ•°æ®
        print("\nğŸ”„ Processing publications...")
        new_pubs, has_updates = self.convert_to_config_format(scholar_pubs, config_data)
        
        # 4. åˆå¹¶æ–°å‡ºç‰ˆç‰©åˆ°ç°æœ‰é…ç½®
        changes_made = has_updates  # å¦‚æœæœ‰æ›´æ–°ç°æœ‰è®ºæ–‡ï¼Œä¹Ÿç®—ä½œæœ‰å˜åŒ–
        for year, pubs in new_pubs.items():
            if year not in config_data['publications']:
                config_data['publications'][year] = []
            
            for new_pub in pubs:
                # å†æ¬¡æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆé˜²æ­¢é‡å¤æ·»åŠ ï¼‰
                exists = any(
                    p['title'].lower() == new_pub['title'].lower()
                    for p in config_data['publications'][year]
                )
                if not exists:
                    config_data['publications'][year].append(new_pub)
                    changes_made = True
        
        # 5. ä¿å­˜é…ç½®
        if changes_made:
            if dry_run:
                print("\nğŸ” DRY RUN - Changes that would be made:")
                print("  (Config file not modified)")
            else:
                print("\nğŸ’¾ Saving updated config...")
                if self.save_config(config_data):
                    print("ğŸ‰ Scholar sync completed successfully!")
                else:
                    return False
        else:
            print("\nâ„¹ï¸  No new publications to add.")
        
        return True

def main():
    parser = argparse.ArgumentParser(description='Google Scholar Publications Sync')
    parser.add_argument('--user-id', '-u', default=SCHOLAR_USER_ID,
                       help='Google Scholar user ID')
    parser.add_argument('--config', '-c', default='config.json',
                       help='Config file path')
    parser.add_argument('--dry-run', '-d', action='store_true',
                       help='Dry run mode (do not modify files)')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='Verbose output')
    
    args = parser.parse_args()
    
    # åˆ›å»ºåŒæ­¥å™¨
    syncer = ScholarSync(user_id=args.user_id, config_path=args.config)
    
    # æ‰§è¡ŒåŒæ­¥
    success = syncer.sync(dry_run=args.dry_run)
    
    if success:
        print("\nâœ… Sync completed successfully!")
        if not args.dry_run:
            print("ğŸ“ Next steps:")
            print("1. Review the changes in config.json")
            print("2. Run 'python build_local.py' to update HTML files")
            print("3. Commit and push changes to GitHub")
        return 0
    else:
        print("\nâŒ Sync failed!")
        return 1

if __name__ == "__main__":
    sys.exit(main()) 